diff --git a/src/components/action_selectors.py b/src/components/action_selectors.py
index d6502a8..5b68921 100644
--- a/src/components/action_selectors.py
+++ b/src/components/action_selectors.py
@@ -63,3 +63,66 @@ class EpsilonGreedyActionSelector():
 
 
 REGISTRY["epsilon_greedy"] = EpsilonGreedyActionSelector
+
+class InfluenceBasedActionSelector():
+
+    def __init__(self, args):
+        self.args = args
+
+        self.schedule = DecayThenFlatSchedule(args.epsilon_start, args.epsilon_finish, args.epsilon_anneal_time,
+                                              decay="linear")
+        self.epsilon = self.schedule.eval(0)
+        self.e_mode = args.e_mode
+
+    def select_action(self, agents_inputs_alone, agents_inputs, avail_actions, t_env, test_mode=False):
+
+        # Assuming agent_inputs is a batch of Q-Values for each agent bav
+        self.epsilon = self.schedule.eval(t_env)
+
+        if test_mode:
+            # Greedy action selection only
+            self.epsilon = 0.0
+
+        if self.e_mode == "negative_sample":
+            # mask actions that are excluded from selection
+            masked_q_values_alone = -agents_inputs_alone.clone()
+            masked_q_values_alone[avail_actions == 0.0] = -float("inf")  # should never be selected!
+            # mask actions that are excluded from selection
+            masked_q_values = agents_inputs.clone()
+            masked_q_values[avail_actions == 0.0] = -float("inf")  # should never be selected!
+        elif self.e_mode == "exclude_max":
+            # mask actions that are excluded from selection
+            masked_q_values_alone = agents_inputs_alone.clone()
+            masked_q_values_alone[avail_actions == 0.0] = -float("inf")  # should never be selected!
+            # mask actions that are excluded from selection
+            masked_q_values = agents_inputs.clone()
+            masked_q_values[avail_actions == 0.0] = -float("inf")  # should never be selected!
+
+            # Get rid off the top value
+            masked_q_values_alone_max = th.argmax(masked_q_values_alone, dim=-1, keepdim=True)
+            masked_q_values_alone_max_oh = th.zeros(masked_q_values_alone.shape).cuda()
+            masked_q_values_alone_max_oh.scatter_(-1, masked_q_values_alone_max, 1)
+            masked_q_values_alone[masked_q_values_alone_max_oh == 1] = -1
+            masked_q_values_alone[masked_q_values_alone_max_oh == 0] = 0
+            masked_q_values_alone = masked_q_values_alone * (th.sum(avail_actions, dim=-1, keepdim=True) != 1)
+            masked_q_values_alone[masked_q_values_alone == -1] = -float("inf")
+            masked_q_values_alone[avail_actions == 0.0] = -float("inf")
+
+
+        random_numbers = th.rand_like(agents_inputs[:, :, 0])
+        pick_random = (random_numbers < self.epsilon).long()
+        random_actions = Categorical(avail_actions.float()).sample().long()
+        #TODO: these numbers are fixed now
+        if t_env > 1000000:
+            random_numbers = th.rand_like(agents_inputs[:, :, 0])
+            pick_alone = (random_numbers < self.args.e_prob).long()
+            alone_actions = Categorical(logits=masked_q_values_alone.float()).sample().long()
+            final_random_actions = pick_alone * alone_actions + (1 - pick_alone) * random_actions
+        else:
+            final_random_actions = random_actions
+
+        picked_actions = pick_random * final_random_actions + (1 - pick_random) * masked_q_values.max(dim=2)[1]
+        return picked_actions
+
+
+REGISTRY["influence"] = InfluenceBasedActionSelector
diff --git a/src/components/transforms.py b/src/components/transforms.py
index 3d267c3..98d72f6 100644
--- a/src/components/transforms.py
+++ b/src/components/transforms.py
@@ -19,4 +19,4 @@ class OneHot(Transform):
         return y_onehot.float()
 
     def infer_output_info(self, vshape_in, dtype_in):
-        return (self.out_dim,), th.float32
\ No newline at end of file
+        return (self.out_dim,), th.float32
diff --git a/src/config/default.yaml b/src/config/default.yaml
index 8a39249..4343f7d 100644
--- a/src/config/default.yaml
+++ b/src/config/default.yaml
@@ -18,8 +18,8 @@ buffer_cpu_only: True # If true we won't keep all of the replay buffer in vram
 
 # --- Logging options ---
 use_tensorboard: False # Log results to tensorboard
-save_model: False # Save the models to disk
-save_model_interval: 2000000 # Save models after this many timesteps
+save_model: True # Save the models to disk
+save_model_interval: 20000 # Save models after this many timesteps
 checkpoint_path: "" # Load a checkpoint from this path
 evaluate: False # Evaluate model for test_nepisode episodes and quit (no training)
 load_step: 0 # Load model trained on this many timesteps (0 if choose max possible)
diff --git a/src/config/envs/sc2.yaml b/src/config/envs/sc2.yaml
index 2e13bb7..84e5ab7 100644
--- a/src/config/envs/sc2.yaml
+++ b/src/config/envs/sc2.yaml
@@ -4,7 +4,7 @@ env_args:
   continuing_episode: False
   difficulty: "7"
   game_version: null
-  map_name: "3m"
+  map_name: ["3m"]
   move_amount: 2
   obs_all_health: True
   obs_instead_of_state: False
diff --git a/src/config/envs/sc2_beta.yaml b/src/config/envs/sc2_beta.yaml
index 03a8f48..480de31 100644
--- a/src/config/envs/sc2_beta.yaml
+++ b/src/config/envs/sc2_beta.yaml
@@ -4,7 +4,7 @@ env_args:
   continuing_episode: False
   difficulty: "7"
   game_version: null
-  map_name: "3m"
+  map_name: ["3m"]
   move_amount: 2
   obs_all_health: True
   obs_instead_of_state: False
diff --git a/src/controllers/__init__.py b/src/controllers/__init__.py
index fe41d02..bcf4ef3 100644
--- a/src/controllers/__init__.py
+++ b/src/controllers/__init__.py
@@ -2,4 +2,14 @@ REGISTRY = {}
 
 from .basic_controller import BasicMAC
 
-REGISTRY["basic_mac"] = BasicMAC
\ No newline at end of file
+REGISTRY["basic_mac"] = BasicMAC
+
+from .basic_controller_interactive import BasicMACInteractive, BasicMACInteractiveRegV1, BasicMACInteractiveRegV2
+
+REGISTRY["basic_mac_interactive"] = BasicMACInteractive
+REGISTRY["basic_mac_interactive_regv1"] = BasicMACInteractiveRegV1
+REGISTRY["basic_mac_interactive_regv2"] = BasicMACInteractiveRegV2
+
+from .basic_controller_influence import BasicMACInfluence
+
+REGISTRY["basic_mac_influence"] = BasicMACInfluence
diff --git a/src/controllers/basic_controller.py b/src/controllers/basic_controller.py
index 7482017..4c9a6d4 100644
--- a/src/controllers/basic_controller.py
+++ b/src/controllers/basic_controller.py
@@ -8,19 +8,29 @@ class BasicMAC:
     def __init__(self, scheme, groups, args):
         self.n_agents = args.n_agents
         self.args = args
-        input_shape = self._get_input_shape(scheme)
-        self._build_agents(input_shape)
+        self.input_shape = self._get_input_shape(scheme)
+        if args.obs_agent_id:
+            self.input_alone_shape = scheme["obs_alone"]["vshape"] + scheme["actions_onehot"]["vshape"][0] + 1
+        else:
+            self.input_alone_shape = scheme["obs_alone"]["vshape"] + scheme["actions_onehot"]["vshape"][0]
+        self._build_agents(self.input_shape, self.input_alone_shape)
         self.agent_output_type = args.agent_output_type
 
         self.action_selector = action_REGISTRY[args.action_selector](args)
 
         self.hidden_states = None
 
-    def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
+        self.test_total = 0
+        self.avg_num_agents_attack = th.zeros(self.n_agents + 1)
+        self.avg_ally_distance = 0
+
+    def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False, env=None):
         # Only select actions for the selected batch elements in bs
         avail_actions = ep_batch["avail_actions"][:, t_ep]
         agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
         chosen_actions = self.action_selector.select_action(agent_outputs[bs], avail_actions[bs], t_env, test_mode=test_mode)
+        # if test_mode:
+        #     self.focus_fire_rate(chosen_actions, ep_batch, t_ep)
         return chosen_actions
 
     def forward(self, ep_batch, t, test_mode=False):
@@ -71,8 +81,8 @@ class BasicMAC:
     def load_models(self, path):
         self.agent.load_state_dict(th.load("{}/agent.th".format(path), map_location=lambda storage, loc: storage))
 
-    def _build_agents(self, input_shape):
-        self.agent = agent_REGISTRY[self.args.agent](input_shape, self.args)
+    def _build_agents(self, input_shape, input_alone_shape):
+        self.agent = agent_REGISTRY[self.args.agent](input_shape, input_alone_shape, self.args)
 
     def _build_inputs(self, batch, t):
         # Assumes homogenous agents with flat observations.
@@ -99,3 +109,40 @@ class BasicMAC:
             input_shape += self.n_agents
 
         return input_shape
+
+    def focus_fire_rate(self, chosen_actions, batch, t):
+        self.test_total += 1
+        n_actions_no_attack = 6
+        #Compute focus fire rate
+        target_id = th.clamp(chosen_actions - n_actions_no_attack, min=-1)
+        max_id = self.args.n_actions - n_actions_no_attack
+        num_agents_attack = []
+        for i in range(max_id):
+            num_agents_attack.append(th.sum(target_id == i).item())
+        #Compute average distance
+        inputs = batch["obs"][:, t]
+        bs = batch.batch_size
+        individual_feats_size = (self.input_shape-self.n_agents-self.input_alone_shape+1) // (self.n_agents - 1) - 4
+        all_feats_size = individual_feats_size + 4
+        n_enemies = (self.input_shape-individual_feats_size-self.n_agents-self.args.n_actions-all_feats_size*(self.n_agents-1)) // all_feats_size
+        enemy_ally_feats = inputs[:, :, -individual_feats_size-all_feats_size*(self.n_agents-1+n_enemies):-individual_feats_size]\
+                            .reshape(inputs.shape[0], inputs.shape[1], self.n_agents-1+n_enemies, -1)
+        #Compute enemy
+        e_shootable = (enemy_ally_feats[:, :, :n_enemies, 0] > 0).long()
+        e_visible = (enemy_ally_feats[:, :, :n_enemies, 1] > 0).long()
+        e_distance = enemy_ally_feats[:, :, :n_enemies, 1]
+        e_average_distance = th.sum(e_distance, dim=1)/(th.sum(e_visible, dim=1) + 1e-6)
+        #Compute ally
+        #Compute enemy
+        a_visible = (enemy_ally_feats[:, :, :n_enemies, 0] > 0).long()
+        a_distance = enemy_ally_feats[:, :, :n_enemies, 1] * a_visible
+        a_average_distance = th.sum(a_distance, dim=1)/(th.sum(a_visible, dim=1) + 1e-6)
+
+        for num_attack in num_agents_attack:
+            self.avg_num_agents_attack[num_attack] += 1
+        self.avg_ally_distance += a_average_distance.mean().item()
+
+        th.set_printoptions(precision=2)
+        print("focus fire rate: ", self.avg_num_agents_attack/self.test_total)
+        print("focus fire rate mean: ", self.avg_num_agents_attack[2:].sum()/self.test_total)
+        print("average distance between agents: ", "%.2f" % (self.avg_ally_distance/self.test_total))
diff --git a/src/envs/multiagentenv.py b/src/envs/multiagentenv.py
index 9c311f3..3ff41af 100644
--- a/src/envs/multiagentenv.py
+++ b/src/envs/multiagentenv.py
@@ -54,6 +54,7 @@ class MultiAgentEnv(object):
     def get_env_info(self):
         env_info = {"state_shape": self.get_state_size(),
                     "obs_shape": self.get_obs_size(),
+                    "obs_alone_shape": self.get_obs_alone_size(),
                     "n_actions": self.get_total_actions(),
                     "n_agents": self.n_agents,
                     "episode_limit": self.episode_limit}
diff --git a/src/learners/__init__.py b/src/learners/__init__.py
index 9310889..3efd9f2 100644
--- a/src/learners/__init__.py
+++ b/src/learners/__init__.py
@@ -1,9 +1,15 @@
 from .q_learner import QLearner
 from .coma_learner import COMALearner
 from .qtran_learner import QLearner as QTranLearner
+from .q_interactive_learner import QInteractiveLearner
+from .q_influence_learner import QInfluenceLearner
+from .q_explore_learner import QExploreLearner
 
 REGISTRY = {}
 
 REGISTRY["q_learner"] = QLearner
 REGISTRY["coma_learner"] = COMALearner
 REGISTRY["qtran_learner"] = QTranLearner
+REGISTRY["q_interactive_learner"] = QInteractiveLearner
+REGISTRY["q_influence_learner"] = QInfluenceLearner
+REGISTRY["q_explore_learner"] = QExploreLearner
diff --git a/src/main.py b/src/main.py
index cd02302..167a6af 100644
--- a/src/main.py
+++ b/src/main.py
@@ -94,6 +94,6 @@ if __name__ == '__main__':
     logger.info("Saving to FileStorageObserver in results/sacred.")
     file_obs_path = os.path.join(results_path, "sacred")
     ex.observers.append(FileStorageObserver.create(file_obs_path))
-
+
     ex.run_commandline(params)
 
diff --git a/src/modules/agents/__init__.py b/src/modules/agents/__init__.py
index 78d9534..b8eb784 100644
--- a/src/modules/agents/__init__.py
+++ b/src/modules/agents/__init__.py
@@ -1,4 +1,11 @@
 REGISTRY = {}
 
-from .rnn_agent import RNNAgent
-REGISTRY["rnn"] = RNNAgent
\ No newline at end of file
+from .rnn_agent import RNNAgent, RNNAttnAgent
+from .rnn_interactive_agent import RNNInteractiveAgent, RNNInteractiveAttnAgentV1, RNNInteractiveAttnAgentV2, RNNInteractiveRegAgent, RNNInteractiveAttnAgent
+REGISTRY["rnn"] = RNNAgent
+REGISTRY["rnn_attn"] = RNNAttnAgent
+REGISTRY["rnn_interactive"] = RNNInteractiveAgent
+REGISTRY["rnn_interactive_reg"] = RNNInteractiveRegAgent
+REGISTRY["rnn_interactive_attnv1"] = RNNInteractiveAttnAgentV1
+REGISTRY["rnn_interactive_attnv2"] = RNNInteractiveAttnAgentV2
+REGISTRY["rnn_interactive_attn"] = RNNInteractiveAttnAgent
diff --git a/src/modules/agents/rnn_agent.py b/src/modules/agents/rnn_agent.py
index 9f80003..95fdb97 100644
--- a/src/modules/agents/rnn_agent.py
+++ b/src/modules/agents/rnn_agent.py
@@ -1,12 +1,11 @@
 import torch.nn as nn
 import torch.nn.functional as F
-
+import torch as th
 
 class RNNAgent(nn.Module):
-    def __init__(self, input_shape, args):
+    def __init__(self, input_shape, input_shape_alone, args):
         super(RNNAgent, self).__init__()
         self.args = args
-
         self.fc1 = nn.Linear(input_shape, args.rnn_hidden_dim)
         self.rnn = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)
         self.fc2 = nn.Linear(args.rnn_hidden_dim, args.n_actions)
@@ -21,3 +20,222 @@ class RNNAgent(nn.Module):
         h = self.rnn(x, h_in)
         q = self.fc2(h)
         return q, h
+
+    def update_n_agents(self, n_agents):
+        pass
+
+class ScaledDotProductAttention(nn.Module):
+    ''' Scaled Dot-Product Attention '''
+
+    def __init__(self, temperature, attn_dropout=0.0):
+        super(ScaledDotProductAttention, self).__init__()
+        self.temperature = temperature
+        self.dropout = nn.Dropout(attn_dropout)
+
+    def forward(self, q, k, v, mask=None):
+        attn = th.matmul(q / self.temperature, k.transpose(2, 3))
+        if mask is not None:
+            attn = attn.masked_fill(mask == 0, -1e9)
+
+        attn = self.dropout(F.softmax(attn, dim=-1))
+        output = th.matmul(attn, v)
+        return output, attn
+
+class Multi_Head_Attention(nn.Module):
+    ''' Multi-Head Attention module '''
+    def __init__(self, n_head, d_model, d_k, d_v, dout, dropout=0., bias=True):
+        super(Multi_Head_Attention, self).__init__()
+
+        self.n_head = n_head
+        self.d_k = d_k
+        self.d_v = d_v
+
+        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=bias)
+        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=bias)
+        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=bias)
+        self.fc = nn.Sequential(nn.Linear(n_head * d_v, n_head * d_v, bias=bias), nn.ReLU(), nn.Linear(n_head * d_v, dout, bias=bias))
+
+        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)
+        self.layer_norm_q = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_k = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_v = nn.LayerNorm(n_head * d_v, eps=1e-6)
+
+
+    def forward(self, q, k, v, mask=None):
+        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head
+        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)
+
+        # Pass through the pre-attention projection: b x lq x (n*dv)
+        # Separate different heads: b x lq x n x dv
+        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
+        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
+        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)
+        residual = q
+
+        # Transpose for attention dot product: b x n x lq x dv
+        q, k, v = self.layer_norm_q(q).transpose(1, 2), self.layer_norm_k(k).transpose(1, 2), self.layer_norm_v(v).transpose(1, 2)
+        if mask is not None:
+            mask = mask.unsqueeze(1)   # For head axis broadcasting.
+        q, attn = self.attention(q, k, v, mask=mask)
+
+        # Transpose to move the head dimension back: b x lq x n x dv
+        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
+        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)
+        q = self.fc(q)
+        return q, residual, attn.squeeze()
+
+class Multi_Head_Attention_2layer(nn.Module):
+    ''' Multi-Head Attention module '''
+    def __init__(self, n_head, d_model, d_k, d_v, dout, dropout=0., bias=True):
+        super(Multi_Head_Attention_2layer, self).__init__()
+
+        self.n_head = n_head
+        self.d_k = d_k
+        self.d_v = d_v
+
+        self.w_qs_1 = nn.Linear(d_model, n_head * d_k, bias=bias)
+        self.w_ks_1 = nn.Linear(d_model, n_head * d_k, bias=bias)
+        self.w_vs_1 = nn.Linear(d_model, n_head * d_v, bias=bias)
+        self.fc_1 = nn.Linear(n_head * d_v, dout, bias=bias)
+
+        self.attention_1 = ScaledDotProductAttention(temperature=d_k ** 0.5)
+        self.layer_norm_q_1 = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_k_1 = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_v_1 = nn.LayerNorm(n_head * d_v, eps=1e-6)
+
+        # 2nd layer of attention
+        self.w_qs_2 = nn.Linear(n_head * d_k, n_head * d_k, bias=bias)
+        self.w_ks_2 = nn.Linear(d_model, n_head * d_k, bias=bias)
+        self.w_vs_2 = nn.Linear(d_model, n_head * d_v, bias=bias)
+        self.fc_2 = nn.Linear(n_head * d_v, dout, bias=bias)
+
+        self.attention_2 = ScaledDotProductAttention(temperature=d_k ** 0.5)
+        self.layer_norm_q_2 = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_k_2 = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_v_2 = nn.LayerNorm(n_head * d_v, eps=1e-6)
+
+
+    def forward(self, q, k, v, mask=None):
+        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head
+        #In this layer, we perform self attention
+        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)
+        # Pass through the pre-attention projection: b x lq x (n*dv)
+        # Separate different heads: b x lq x n x dv
+        q_ = self.w_qs_1(q).view(sz_b, len_q, n_head, d_k)
+        k_ = self.w_ks_1(k).view(sz_b, len_k, n_head, d_k)
+        v_ = self.w_vs_1(v).view(sz_b, len_v, n_head, d_v)
+        residual1 = q_
+
+        # Transpose for attention dot product: b x n x lq x dv
+        q_, k_, v_ = self.layer_norm_q_1(q_).transpose(1, 2), self.layer_norm_k_1(k_).transpose(1, 2), self.layer_norm_v_1(v_).transpose(1, 2)
+        if mask is not None:
+            mask = mask.unsqueeze(1)   # For head axis broadcasting.
+        q_, attn1 = self.attention_1(q_, k_, v_, mask=mask)
+
+        # Transpose to move the head dimension back: b x lq x n x dv
+        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
+        q_ = q_.transpose(1, 2).contiguous().view(sz_b, len_q, -1)
+        q_ = self.fc_1(q_)
+
+        # In second layer we use attention
+        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)
+        # Pass through the pre-attention projection: b x lq x (n*dv)
+        # Separate different heads: b x lq x n x dv
+        q_ = self.w_qs_2(q_).view(sz_b, len_q, n_head, d_k)
+        k_ = self.w_ks_2(k).view(sz_b, len_k, n_head, d_k)
+        v_ = self.w_vs_2(v).view(sz_b, len_v, n_head, d_v)
+        residual2 = q_
+
+        # Transpose for attention dot product: b x n x lq x dv
+        q_, k_, v_ = self.layer_norm_q_2(q_).transpose(1, 2), self.layer_norm_k_2(k_).transpose(1, 2), self.layer_norm_v_2(v_).transpose(1, 2)
+        if mask is not None:
+            mask = mask.unsqueeze(1)   # For head axis broadcasting.
+        q_, attn2 = self.attention_2(q_, k_, v_, mask=mask)
+
+        # Transpose to move the head dimension back: b x lq x n x dv
+        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
+        q_ = q_.transpose(1, 2).contiguous().view(sz_b, len_q, -1)
+        q_ = self.fc_2(q_)
+        return q_, th.cat((residual1, residual2), dim=-1), attn2.squeeze()
+
+
+class SelfAttn(nn.Module):
+    def __init__(self, input_shape, input_shape_alone, args):
+        super(SelfAttn, self).__init__()
+        self.args = args
+        if args.obs_agent_id:
+            self.individual_feats_size = (input_shape-args.n_agents-input_shape_alone+1) // (args.n_agents - 1) - 4
+        else:
+            self.individual_feats_size = (input_shape-input_shape_alone) // (args.n_agents - 1) - 4
+        self.all_feats_size = self.individual_feats_size + 4
+        self.n_enemies = (input_shape-self.individual_feats_size-self.args.n_agents-self.args.n_actions-self.all_feats_size*(self.args.n_agents-1)) // self.all_feats_size
+        self.self_relative = th.tensor([1, 0, 0, 0], device=self.args.device).float().reshape(1, 1, -1)
+        if args.attn_layers == 1:
+            self.a_self_attn = Multi_Head_Attention(1, self.all_feats_size, args.attn_embed_dim, args.attn_embed_dim, args.attn_embed_dim)
+        elif args.attn_layers == 2:
+            self.a_self_attn = Multi_Head_Attention_2layer(1, self.all_feats_size, args.attn_embed_dim, args.attn_embed_dim, args.attn_embed_dim)
+        self.n_agents = args.n_agents
+
+    def forward(self, inputs):
+        if self.args.obs_agent_id:
+            bs = inputs.shape[0]
+            # World features
+            world_feats = inputs[:, :-self.individual_feats_size-self.n_agents-self.args.n_actions-self.all_feats_size*(self.n_agents-1+self.n_enemies)]
+            action_id_feats = inputs[:, -self.n_agents-self.args.n_actions:]
+            self_feats = inputs[:, -self.n_agents-self.args.n_actions-self.individual_feats_size:-self.n_agents-self.args.n_actions].reshape(bs, 1, -1)
+            self_feats = th.cat((self.self_relative.expand((bs, 1, 4)), self_feats), dim=-1)
+            #Ally features
+            ally_feats = inputs[:, -self.individual_feats_size-self.n_agents-self.args.n_actions-self.all_feats_size*(self.n_agents-1):-self.n_agents-self.args.n_actions-self.individual_feats_size].reshape(bs, self.n_agents-1, -1)
+            ally_feats, self_feats_a, _ = self.a_self_attn(self_feats, ally_feats, ally_feats)
+            ally_self_feats = th.cat((ally_feats.reshape(bs, -1), self_feats_a.reshape(bs, -1)), dim=-1)
+            #Enemy features
+            enemy_feats = inputs[:, -self.individual_feats_size-self.n_agents-self.args.n_actions-self.all_feats_size*(self.n_agents-1+self.n_enemies):-self.individual_feats_size-self.n_agents-self.args.n_actions-self.all_feats_size*(self.n_agents-1)].reshape(bs, self.n_enemies, -1)
+            enemy_self_feats = enemy_feats.reshape(bs, -1)
+            #Concat everything
+            inputs = th.cat((world_feats, enemy_self_feats, ally_self_feats, action_id_feats), dim=-1)
+        else:
+            bs = inputs.shape[0]
+            # World features
+            world_feats = inputs[:, :-self.individual_feats_size-self.args.n_actions-self.all_feats_size*(self.n_agents-1+self.n_enemies)]
+            action_id_feats = inputs[:, -self.args.n_actions:]
+            self_feats = inputs[:, -self.args.n_actions-self.individual_feats_size:-self.args.n_actions].reshape(bs, 1, -1)
+            self_feats = th.cat((self.self_relative.expand((bs, 1, 4)), self_feats), dim=-1)
+            #Ally features
+            ally_feats = inputs[:, -self.individual_feats_size-self.args.n_actions-self.all_feats_size*(self.n_agents-1):-self.args.n_actions-self.individual_feats_size].reshape(bs, self.n_agents-1, -1)
+            ally_feats, self_feats_a, _ = self.a_self_attn(self_feats, ally_feats, ally_feats)
+            ally_self_feats = th.cat((ally_feats.reshape(bs, -1), self_feats_a.reshape(bs, -1)), dim=-1)
+            #Enemy features
+            enemy_feats = inputs[:, -self.individual_feats_size-self.args.n_actions-self.all_feats_size*(self.n_agents-1+self.n_enemies):-self.individual_feats_size-self.args.n_actions-self.all_feats_size*(self.n_agents-1)].reshape(bs, self.n_enemies, -1)
+            enemy_self_feats = enemy_feats.reshape(bs, -1)
+            #Concat everything
+            inputs = th.cat((world_feats, enemy_self_feats, ally_self_feats, action_id_feats), dim=-1)
+        return inputs
+
+class RNNAttnAgent(nn.Module):
+    def __init__(self, input_shape, input_shape_alone, args):
+        super(RNNAttnAgent, self).__init__()
+        if args.obs_agent_id:
+            self.individual_feats_size = (input_shape-args.n_agents-input_shape_alone+1) // (args.n_agents - 1) - 4
+        else:
+            self.individual_feats_size = (input_shape-input_shape_alone) // (args.n_agents - 1) - 4
+        self.all_feats_size = self.individual_feats_size + 4
+        self.self_attn = SelfAttn(input_shape, input_shape_alone, args)
+        if args.attn_layers == 1:
+            self.agent = RNNAgent(input_shape+args.attn_embed_dim*2-self.individual_feats_size-(args.n_agents-1)*self.all_feats_size, input_shape_alone, args)
+        elif args.attn_layers == 2:
+            self.agent = RNNAgent(input_shape+args.attn_embed_dim*3-self.individual_feats_size-(args.n_agents-1)*self.all_feats_size, input_shape_alone, args)
+        self.args = args
+        self.n_agents = args.n_agents
+
+    def init_hidden(self):
+        # make hidden states on same device as model
+        return self.agent.init_hidden()
+
+    def forward(self, inputs, hidden_state):
+        inputs = self.self_attn(inputs)
+        q, h = self.agent(inputs, hidden_state)
+        return q, h
+
+    def update_n_agents(self, n_agents):
+        self.n_agents = n_agents
+        self.self_attn.n_agents = n_agents
diff --git a/src/modules/critics/coma.py b/src/modules/critics/coma.py
index ddc9ab2..ab9e744 100644
--- a/src/modules/critics/coma.py
+++ b/src/modules/critics/coma.py
@@ -67,4 +67,4 @@ class COMACritic(nn.Module):
         input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
         # agent id
         input_shape += self.n_agents
-        return input_shape
\ No newline at end of file
+        return input_shape
diff --git a/src/modules/mixers/qmix.py b/src/modules/mixers/qmix.py
index 7c98c13..90bce85 100644
--- a/src/modules/mixers/qmix.py
+++ b/src/modules/mixers/qmix.py
@@ -3,6 +3,129 @@ import torch.nn as nn
 import torch.nn.functional as F
 import numpy as np
 
+class ScaledDotProductAttention(nn.Module):
+    ''' Scaled Dot-Product Attention '''
+
+    def __init__(self, temperature, attn_dropout=0.0):
+        super(ScaledDotProductAttention, self).__init__()
+        self.temperature = temperature
+        self.dropout = nn.Dropout(attn_dropout)
+
+    def forward(self, q, k, v, mask=None):
+        attn = th.matmul(q / self.temperature, k.transpose(2, 3))
+        if mask is not None:
+            attn = attn.masked_fill(mask == 0, -1e9)
+
+        attn = self.dropout(F.softmax(attn, dim=-1))
+        output = th.matmul(attn, v)
+        return output, attn
+
+class Multi_Head_Attention(nn.Module):
+    ''' Multi-Head Attention module '''
+    def __init__(self, n_head, d_model, d_k, d_v, dout, dropout=0., bias=True):
+        super(Multi_Head_Attention, self).__init__()
+
+        self.n_head = n_head
+        self.d_k = d_k
+        self.d_v = d_v
+
+        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=bias)
+        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=bias)
+        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=bias)
+        self.fc = nn.Linear(n_head * d_v, dout, bias=bias)
+
+        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)
+        self.layer_norm_q = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_k = nn.LayerNorm(n_head * d_k, eps=1e-6)
+        self.layer_norm_v = nn.LayerNorm(n_head * d_v, eps=1e-6)
+
+
+    def forward(self, q, k, v, mask=None):
+        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head
+        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)
+
+        # Pass through the pre-attention projection: b x lq x (n*dv)
+        # Separate different heads: b x lq x n x dv
+        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)
+        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)
+        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)
+        residual = q
+
+        # Transpose for attention dot product: b x n x lq x dv
+        q, k, v = self.layer_norm_q(q).transpose(1, 2), self.layer_norm_k(k).transpose(1, 2), self.layer_norm_v(v).transpose(1, 2)
+        if mask is not None:
+            mask = mask.unsqueeze(1)   # For head axis broadcasting.
+        q, attn = self.attention(q, k, v, mask=mask)
+
+        # Transpose to move the head dimension back: b x lq x n x dv
+        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)
+        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)
+        q = self.fc(q)
+        return q, residual, attn.squeeze()
+
+
+class SelfAttnV1(nn.Module):
+    def __init__(self, state_dim, input_shape, input_shape_alone, args):
+        super(SelfAttnV1, self).__init__()
+        self.args = args
+        self.n_agents = self.args.n_agents
+        self.n_enemies = self.args.n_actions - 6 # 6 is the non-attack actions
+        self.action_size = self.n_agents * self.args.n_actions
+        self.a_individual_feats_size = (input_shape-args.n_agents-input_shape_alone+1) // (args.n_agents - 1) - 1
+        self.e_individual_feats_size = (state_dim - self.action_size - self.a_individual_feats_size * self.n_agents) // self.n_enemies
+        self.a_self_attn = Multi_Head_Attention(1, self.a_individual_feats_size, args.attn_embed_dim, args.attn_embed_dim, args.attn_embed_dim)
+        self.e_cooldown = th.zeros((1, 1, 1), device=self.args.device).float()
+
+    def forward(self, inputs):
+        bs = inputs.shape[0]
+        #Actions
+        actions = inputs[:, -self.action_size:]
+        #Ally feats
+        ally_feats = inputs[:, :-self.action_size-self.n_enemies*self.e_individual_feats_size]
+        ally_feats = ally_feats.reshape(bs, self.n_agents, -1)
+        self_feats = ally_feats[:, [0], :]
+        ally_feats = ally_feats[:, 1:, :]
+        ally_feats, self_feats_a, _ = self.a_self_attn(self_feats, ally_feats, ally_feats)
+        ally_self_feats = th.cat((ally_feats.reshape(bs, -1), self_feats_a.reshape(bs, -1)), dim=-1)
+        #Enemy feats
+        enemy_feats = inputs[:, -self.action_size-self.n_enemies*self.e_individual_feats_size:-self.action_size]
+        enemy_feats = enemy_feats.reshape(bs, self.n_enemies, -1)
+        enemy_self_feats = enemy_feats.reshape(bs, -1)
+        x = th.cat((ally_self_feats, enemy_self_feats, actions), dim=-1)
+        return x
+
+class SelfAttnV2(nn.Module):
+    def __init__(self, state_dim, input_shape, input_shape_alone, args):
+        super(SelfAttnV2, self).__init__()
+        self.args = args
+        self.n_agents = self.args.n_agents
+        self.n_enemies = self.args.n_actions - 6 # 6 is the non-attack actions
+        self.action_size = self.n_agents * self.args.n_actions
+        self.a_individual_feats_size = (input_shape-args.n_agents-input_shape_alone+1) // (args.n_agents - 1) - 1
+        self.e_individual_feats_size = (state_dim - self.action_size - self.a_individual_feats_size * self.n_agents) // self.n_enemies
+        self.a_self_attn = Multi_Head_Attention(1, self.a_individual_feats_size, args.attn_embed_dim, args.attn_embed_dim, args.attn_embed_dim)
+        self.e_self_attn = Multi_Head_Attention(1, self.a_individual_feats_size, args.attn_embed_dim, args.attn_embed_dim, args.attn_embed_dim)
+        self.e_cooldown = th.zeros((1, 1, 1), device=self.args.device).float()
+
+    def forward(self, inputs):
+        bs = inputs.shape[0]
+        #Actions
+        actions = inputs[:, -self.action_size:]
+        #Ally feats
+        ally_feats = inputs[:, :-self.action_size-self.n_enemies*self.e_individual_feats_size]
+        ally_feats = ally_feats.reshape(bs, self.n_agents, -1)
+        self_feats = ally_feats[:, [0], :]
+        ally_feats = ally_feats[:, 1:, :]
+        ally_feats, self_feats_a, _ = self.a_self_attn(self_feats, ally_feats, ally_feats)
+        ally_self_feats = th.cat((ally_feats.reshape(bs, -1), self_feats_a.reshape(bs, -1)), dim=-1)
+        #Enemy feats
+        enemy_feats = inputs[:, -self.action_size-self.n_enemies*self.e_individual_feats_size:-self.action_size]
+        enemy_feats = enemy_feats.reshape(bs, self.n_enemies, -1)
+        enemy_feats = th.cat((enemy_feats[:, :, [0]], self.e_cooldown.expand(bs, self.n_enemies, 1), enemy_feats[:, :, 1:]), dim=-1)
+        enemy_feats, self_feats_e, _ = self.e_self_attn(self_feats, enemy_feats, enemy_feats)
+        enemy_self_feats = th.cat((enemy_feats.reshape(bs, -1), self_feats_e.reshape(bs, -1)), dim=-1)
+        x = th.cat((ally_self_feats, enemy_self_feats, actions), dim=-1)
+        return x
 
 class QMixer(nn.Module):
     def __init__(self, args):
@@ -58,3 +181,140 @@ class QMixer(nn.Module):
         # Reshape and return
         q_tot = y.view(bs, -1, 1)
         return q_tot
+
+
+class QAttnMixerV1(nn.Module):
+    def __init__(self, args, input_shape=0, input_shape_alone=0):
+        super(QAttnMixerV1, self).__init__()
+
+        self.args = args
+        self.n_agents = args.n_agents
+        self.state_dim = int(np.prod(args.state_shape))
+        self.self_attn = SelfAttnV1(self.state_dim, input_shape, input_shape_alone, args)
+        self.embed_dim = args.mixing_embed_dim
+        self.n_enemies = self.args.n_actions - 6 # 6 is the non-attack actions
+        self.action_size = self.n_agents * self.args.n_actions
+        self.a_individual_feats_size = (input_shape-args.n_agents-input_shape_alone+1) // (args.n_agents - 1) - 1
+        self.e_individual_feats_size = (self.state_dim - self.action_size - self.a_individual_feats_size * self.n_agents) // self.n_enemies
+
+        if getattr(args, "hypernet_layers", 1) == 1:
+            self.hyper_w_1 = nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size+args.attn_embed_dim*2, self.embed_dim)
+            self.hyper_w_final = nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size+args.attn_embed_dim*2, self.embed_dim)
+        elif getattr(args, "hypernet_layers", 1) == 2:
+            hypernet_embed = self.args.hypernet_embed
+            self.hyper_w_1 = nn.Sequential(nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size+args.attn_embed_dim*2, hypernet_embed),
+                                           nn.ReLU(),
+                                           nn.Linear(hypernet_embed, self.embed_dim))
+            self.hyper_w_final = nn.Sequential(nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size+args.attn_embed_dim*2, hypernet_embed),
+                                           nn.ReLU(),
+                                           nn.Linear(hypernet_embed, self.embed_dim))
+        elif getattr(args, "hypernet_layers", 1) > 2:
+            raise Exception("Sorry >2 hypernet layers is not implemented!")
+        else:
+            raise Exception("Error setting number of hypernet layers.")
+
+        # State dependent bias for hidden layer
+        self.hyper_b_1 = nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size+args.attn_embed_dim*2, self.embed_dim)
+
+        # V(s) instead of a bias for the last layers
+        self.V = nn.Sequential(nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size+args.attn_embed_dim*2, self.embed_dim),
+                               nn.ReLU(),
+                               nn.Linear(self.embed_dim, 1))
+
+    def forward(self, agent_qs, states):
+        bs = agent_qs.size(0)
+        states = states.reshape(-1, self.state_dim).unsqueeze(1)
+        states = states.expand(-1, self.n_agents, self.state_dim)
+        for agent_i in range(self.n_agents):
+            states[:, agent_i, :-self.action_size-self.n_enemies*self.e_individual_feats_size] = th.cat((states[:, agent_i, -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i)*self.a_individual_feats_size:\
+                -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i-1)*self.a_individual_feats_size], states[:, agent_i, -self.action_size-self.n_enemies*self.e_individual_feats_size-self.n_agents*self.a_individual_feats_size:\
+                -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i)*self.a_individual_feats_size], states[:, agent_i, -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i-1)*self.a_individual_feats_size:\
+                -self.action_size-self.n_enemies*self.e_individual_feats_size]), dim=-1)
+        states = states.reshape(-1, self.state_dim)
+        states = self.self_attn(states)
+
+        agent_qs = agent_qs.view(-1, 1, self.n_agents)
+        # First layer
+        w1 = th.abs(self.hyper_w_1(states)) # bs*n_agents, -1
+        b1 = self.hyper_b_1(states) # bs*n_agents, -1
+        w1 = w1.view(-1, self.n_agents, self.embed_dim)
+        b1 = b1.view(-1, self.n_agents, self.embed_dim)
+        hidden = F.elu(th.bmm(agent_qs, w1) + b1.mean(1, keepdim=True))
+        # Second layer
+        w_final = th.abs(self.hyper_w_final(states))
+        w_final = w_final.view(-1, self.n_agents, self.embed_dim, 1).mean(1)
+        # State-dependent bias
+        v = self.V(states).view(-1, self.n_agents, 1, 1).mean(1)
+        # Compute final output
+        y = th.bmm(hidden, w_final) + v
+        # Reshape and return
+        q_tot = y.view(bs, -1, 1)
+        return q_tot
+
+class QAttnMixerV2(nn.Module):
+    def __init__(self, args, input_shape=0, input_shape_alone=0):
+        super(QAttnMixerV2, self).__init__()
+
+        self.args = args
+        self.n_agents = args.n_agents
+        self.state_dim = int(np.prod(args.state_shape))
+        self.self_attn = SelfAttnV2(self.state_dim, input_shape, input_shape_alone, args)
+        self.embed_dim = args.mixing_embed_dim
+        self.n_enemies = self.args.n_actions - 6 # 6 is the non-attack actions
+        self.action_size = self.n_agents * self.args.n_actions
+        self.a_individual_feats_size = (input_shape-args.n_agents-input_shape_alone+1) // (args.n_agents - 1) - 1
+        self.e_individual_feats_size = (self.state_dim - self.action_size - self.a_individual_feats_size * self.n_agents) // self.n_enemies
+
+        if getattr(args, "hypernet_layers", 1) == 1:
+            self.hyper_w_1 = nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size-self.n_enemies*self.e_individual_feats_size+args.attn_embed_dim*4, self.embed_dim)
+            self.hyper_w_final = nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size-self.n_enemies*self.e_individual_feats_size+args.attn_embed_dim*4, self.embed_dim)
+        elif getattr(args, "hypernet_layers", 1) == 2:
+            hypernet_embed = self.args.hypernet_embed
+            self.hyper_w_1 = nn.Sequential(nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size-self.n_enemies*self.e_individual_feats_size+args.attn_embed_dim*4, hypernet_embed),
+                                           nn.ReLU(),
+                                           nn.Linear(hypernet_embed, self.embed_dim))
+            self.hyper_w_final = nn.Sequential(nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size-self.n_enemies*self.e_individual_feats_size+args.attn_embed_dim*4, hypernet_embed),
+                                           nn.ReLU(),
+                                           nn.Linear(hypernet_embed, self.embed_dim))
+        elif getattr(args, "hypernet_layers", 1) > 2:
+            raise Exception("Sorry >2 hypernet layers is not implemented!")
+        else:
+            raise Exception("Error setting number of hypernet layers.")
+
+        # State dependent bias for hidden layer
+        self.hyper_b_1 = nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size-self.n_enemies*self.e_individual_feats_size+args.attn_embed_dim*4, self.embed_dim)
+
+        # V(s) instead of a bias for the last layers
+        self.V = nn.Sequential(nn.Linear(self.state_dim-self.n_agents*self.a_individual_feats_size-self.n_enemies*self.e_individual_feats_size+args.attn_embed_dim*4, self.embed_dim),
+                               nn.ReLU(),
+                               nn.Linear(self.embed_dim, 1))
+
+    def forward(self, agent_qs, states):
+        bs = agent_qs.size(0)
+        states = states.reshape(-1, self.state_dim).unsqueeze(1)
+        states = states.expand(-1, self.n_agents, self.state_dim)
+        for agent_i in range(self.n_agents):
+            states[:, agent_i, :-self.action_size-self.n_enemies*self.e_individual_feats_size] = th.cat((states[:, agent_i, -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i)*self.a_individual_feats_size:\
+                -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i-1)*self.a_individual_feats_size], states[:, agent_i, -self.action_size-self.n_enemies*self.e_individual_feats_size-self.n_agents*self.a_individual_feats_size:\
+                -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i)*self.a_individual_feats_size], states[:, agent_i, -self.action_size-self.n_enemies*self.e_individual_feats_size-(self.n_agents-agent_i-1)*self.a_individual_feats_size:\
+                -self.action_size-self.n_enemies*self.e_individual_feats_size]), dim=-1)
+        states = states.reshape(-1, self.state_dim)
+        states = self.self_attn(states)
+
+        agent_qs = agent_qs.view(-1, 1, self.n_agents)
+        # First layer
+        w1 = th.abs(self.hyper_w_1(states)) # bs*n_agents, -1
+        b1 = self.hyper_b_1(states) # bs*n_agents, -1
+        w1 = w1.view(-1, self.n_agents, self.embed_dim)
+        b1 = b1.view(-1, self.n_agents, self.embed_dim)
+        hidden = F.elu(th.bmm(agent_qs, w1) + b1.mean(1, keepdim=True))
+        # Second layer
+        w_final = th.abs(self.hyper_w_final(states))
+        w_final = w_final.view(-1, self.n_agents, self.embed_dim, 1).mean(1)
+        # State-dependent bias
+        v = self.V(states).view(-1, self.n_agents, 1, 1).mean(1)
+        # Compute final output
+        y = th.bmm(hidden, w_final) + v
+        # Reshape and return
+        q_tot = y.view(bs, -1, 1)
+        return q_tot
diff --git a/src/modules/mixers/vdn.py b/src/modules/mixers/vdn.py
index fc05b63..700f2e7 100644
--- a/src/modules/mixers/vdn.py
+++ b/src/modules/mixers/vdn.py
@@ -7,4 +7,4 @@ class VDNMixer(nn.Module):
         super(VDNMixer, self).__init__()
 
     def forward(self, agent_qs, batch):
-        return th.sum(agent_qs, dim=2, keepdim=True)
\ No newline at end of file
+        return th.sum(agent_qs, dim=2, keepdim=True)
diff --git a/src/run.py b/src/run.py
index 7ba071a..ad78ac2 100644
--- a/src/run.py
+++ b/src/run.py
@@ -64,7 +64,6 @@ def run(_run, _config, _log):
 
 
 def evaluate_sequential(args, runner):
-
     for _ in range(args.test_nepisode):
         runner.run(test_mode=True)
 
@@ -92,7 +91,10 @@ def run_sequential(args, logger):
         "avail_actions": {"vshape": (env_info["n_actions"],), "group": "agents", "dtype": th.int},
         "reward": {"vshape": (1,)},
         "terminated": {"vshape": (1,), "dtype": th.uint8},
+        # This is added for Q_alone
+        "obs_alone": {"vshape": env_info["obs_alone_shape"], "group": "agents"},
     }
+
     groups = {
         "agents": args.n_agents
     }
@@ -146,6 +148,31 @@ def run_sequential(args, logger):
         runner.t_env = timestep_to_load
 
         if args.evaluate or args.save_replay:
+            runner.env.reset(test_mode=True)
+            # Set up schemes and groups here
+            env_info = runner.get_env_info()
+            # Default/Base scheme
+            scheme = {
+                "state": {"vshape": env_info["state_shape"]},
+                "obs": {"vshape": env_info["obs_shape"], "group": "agents"},
+                "actions": {"vshape": (1,), "group": "agents", "dtype": th.long},
+                "avail_actions": {"vshape": (env_info["n_actions"],), "group": "agents", "dtype": th.int},
+                "reward": {"vshape": (1,)},
+                "terminated": {"vshape": (1,), "dtype": th.uint8},
+                # This is added for Q_alone
+                "obs_alone": {"vshape": env_info["obs_alone_shape"], "group": "agents"},
+            }
+            groups = {
+                "agents": env_info["n_agents"]
+            }
+            preprocess = {
+                "actions": ("actions_onehot", [OneHot(out_dim=args.n_actions)])
+            }
+            # Give runner the scheme
+            runner.setup(scheme=scheme, groups=groups, preprocess=preprocess, mac=mac)
+            mac.n_agents = env_info["n_agents"]
+            mac.agent.update_n_agents(env_info["n_agents"])
+
             evaluate_sequential(args, runner)
             return
 
@@ -188,12 +215,63 @@ def run_sequential(args, logger):
             last_time = time.time()
 
             last_test_T = runner.t_env
+
+            runner.env.reset(test_mode=True)
+            # Set up schemes and groups here
+            env_info = runner.get_env_info()
+            # Default/Base scheme
+            scheme = {
+                "state": {"vshape": env_info["state_shape"]},
+                "obs": {"vshape": env_info["obs_shape"], "group": "agents"},
+                "actions": {"vshape": (1,), "group": "agents", "dtype": th.long},
+                "avail_actions": {"vshape": (env_info["n_actions"],), "group": "agents", "dtype": th.int},
+                "reward": {"vshape": (1,)},
+                "terminated": {"vshape": (1,), "dtype": th.uint8},
+                # This is added for Q_alone
+                "obs_alone": {"vshape": env_info["obs_alone_shape"], "group": "agents"},
+            }
+            groups = {
+                "agents": env_info["n_agents"]
+            }
+            preprocess = {
+                "actions": ("actions_onehot", [OneHot(out_dim=args.n_actions)])
+            }
+            # Give runner the scheme
+            runner.setup(scheme=scheme, groups=groups, preprocess=preprocess, mac=mac)
+            mac.n_agents = env_info["n_agents"]
+            mac.agent.update_n_agents(env_info["n_agents"])
+
             for _ in range(n_test_runs):
                 runner.run(test_mode=True)
 
+            runner.env.reset(test_mode=False)
+            # Set up schemes and groups here
+            env_info = runner.get_env_info()
+            # Default/Base scheme
+            scheme = {
+                "state": {"vshape": env_info["state_shape"]},
+                "obs": {"vshape": env_info["obs_shape"], "group": "agents"},
+                "actions": {"vshape": (1,), "group": "agents", "dtype": th.long},
+                "avail_actions": {"vshape": (env_info["n_actions"],), "group": "agents", "dtype": th.int},
+                "reward": {"vshape": (1,)},
+                "terminated": {"vshape": (1,), "dtype": th.uint8},
+                # This is added for Q_alone
+                "obs_alone": {"vshape": env_info["obs_alone_shape"], "group": "agents"},
+            }
+            groups = {
+                "agents": args.n_agents
+            }
+            preprocess = {
+                "actions": ("actions_onehot", [OneHot(out_dim=args.n_actions)])
+            }
+            # Give runner the scheme
+            runner.setup(scheme=scheme, groups=groups, preprocess=preprocess, mac=mac)
+            mac.n_agents = env_info["n_agents"]
+            mac.agent.update_n_agents(env_info["n_agents"])
+
         if args.save_model and (runner.t_env - model_save_time >= args.save_model_interval or model_save_time == 0):
             model_save_time = runner.t_env
-            save_path = os.path.join(args.local_results_path, "models", args.unique_token, str(runner.t_env))
+            save_path = os.path.join(args.local_results_path, "models", '-'.join(args.env_args['map_name']), args.unique_token, str(runner.t_env))
             #"results/models/{}".format(unique_token)
             os.makedirs(save_path, exist_ok=True)
             logger.console_logger.info("Saving models to {}".format(save_path))
diff --git a/src/runners/episode_runner.py b/src/runners/episode_runner.py
index 6fc41e8..5ed97b2 100644
--- a/src/runners/episode_runner.py
+++ b/src/runners/episode_runner.py
@@ -40,13 +40,13 @@ class EpisodeRunner:
     def close_env(self):
         self.env.close()
 
-    def reset(self):
+    def reset(self, test_mode=False):
         self.batch = self.new_batch()
-        self.env.reset()
+        self.env.reset(test_mode)
         self.t = 0
 
     def run(self, test_mode=False):
-        self.reset()
+        self.reset(test_mode)
 
         terminated = False
         episode_return = 0
@@ -57,14 +57,15 @@ class EpisodeRunner:
             pre_transition_data = {
                 "state": [self.env.get_state()],
                 "avail_actions": [self.env.get_avail_actions()],
-                "obs": [self.env.get_obs()]
+                "obs": [self.env.get_obs()],
+                "obs_alone": [self.env.get_obs_alone()],
             }
 
             self.batch.update(pre_transition_data, ts=self.t)
 
             # Pass the entire batch of experiences up till now to the agents
             # Receive the actions for each agent at this timestep in a batch of size 1
-            actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, test_mode=test_mode)
+            actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, test_mode=test_mode, env=self.env)
 
             reward, terminated, env_info = self.env.step(actions[0])
             episode_return += reward
@@ -82,12 +83,13 @@ class EpisodeRunner:
         last_data = {
             "state": [self.env.get_state()],
             "avail_actions": [self.env.get_avail_actions()],
-            "obs": [self.env.get_obs()]
+            "obs": [self.env.get_obs()],
+            "obs_alone": [self.env.get_obs_alone()],
         }
         self.batch.update(last_data, ts=self.t)
 
         # Select actions in the last stored state
-        actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, test_mode=test_mode)
+        actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, test_mode=test_mode, env=self.env)
         self.batch.update({"actions": actions}, ts=self.t)
 
         cur_stats = self.test_stats if test_mode else self.train_stats
diff --git a/src/runners/parallel_runner.py b/src/runners/parallel_runner.py
index bf06dec..c59701a 100644
--- a/src/runners/parallel_runner.py
+++ b/src/runners/parallel_runner.py
@@ -58,17 +58,18 @@ class ParallelRunner:
         for parent_conn in self.parent_conns:
             parent_conn.send(("close", None))
 
-    def reset(self):
+    def reset(self, test_mode=False):
         self.batch = self.new_batch()
 
         # Reset the envs
         for parent_conn in self.parent_conns:
-            parent_conn.send(("reset", None))
+            parent_conn.send(("reset", test_mode))
 
         pre_transition_data = {
             "state": [],
             "avail_actions": [],
-            "obs": []
+            "obs": [],
+            "obs_alone": [],
         }
         # Get the obs, state and avail_actions back
         for parent_conn in self.parent_conns:
@@ -76,6 +77,7 @@ class ParallelRunner:
             pre_transition_data["state"].append(data["state"])
             pre_transition_data["avail_actions"].append(data["avail_actions"])
             pre_transition_data["obs"].append(data["obs"])
+            pre_transition_data["obs_alone"].append(data["obs_alone"])
 
         self.batch.update(pre_transition_data, ts=0)
 
@@ -83,7 +85,7 @@ class ParallelRunner:
         self.env_steps_this_run = 0
 
     def run(self, test_mode=False):
-        self.reset()
+        self.reset(test_mode)
 
         all_terminated = False
         episode_returns = [0 for _ in range(self.batch_size)]
@@ -129,7 +131,8 @@ class ParallelRunner:
             pre_transition_data = {
                 "state": [],
                 "avail_actions": [],
-                "obs": []
+                "obs": [],
+                "obs_alone": [],
             }
 
             # Receive data back for each unterminated env
@@ -156,6 +159,7 @@ class ParallelRunner:
                     pre_transition_data["state"].append(data["state"])
                     pre_transition_data["avail_actions"].append(data["avail_actions"])
                     pre_transition_data["obs"].append(data["obs"])
+                    pre_transition_data["obs_alone"].append(data["obs_alone"])
 
             # Add post_transiton data into the batch
             self.batch.update(post_transition_data, bs=envs_not_terminated, ts=self.t, mark_filled=False)
@@ -223,22 +227,25 @@ def env_worker(remote, env_fn):
             state = env.get_state()
             avail_actions = env.get_avail_actions()
             obs = env.get_obs()
+            obs_alone = env.get_obs_alone()
             remote.send({
                 # Data for the next timestep needed to pick an action
                 "state": state,
                 "avail_actions": avail_actions,
                 "obs": obs,
+                "obs_alone": obs_alone,
                 # Rest of the data for the current timestep
                 "reward": reward,
                 "terminated": terminated,
                 "info": env_info
             })
         elif cmd == "reset":
-            env.reset()
+            env.reset(data)
             remote.send({
                 "state": env.get_state(),
                 "avail_actions": env.get_avail_actions(),
-                "obs": env.get_obs()
+                "obs": env.get_obs(),
+                "obs_alone": env.get_obs_alone(),
             })
         elif cmd == "close":
             env.close()
